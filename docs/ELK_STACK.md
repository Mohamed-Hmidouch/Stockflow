# üìä STACK ELK (Elasticsearch, Logstash, Kibana)

## üìã Vue d'ensemble

La stack ELK a √©t√© ajout√©e au projet pour centraliser et visualiser les logs de l'application StockGestion.

### üîß Composants

1. **Elasticsearch** (Port 9200)
   - Base de donn√©es NoSQL pour stocker les logs
   - Indexation et recherche rapide
   - Stockage: volume Docker `es_data`

2. **Logstash** (Port 5044)
   - Pipeline de traitement des logs
   - R√©ception depuis l'application (TCP JSON)
   - Envoi vers Elasticsearch

3. **Kibana** (Port 5601)
   - Interface web de visualisation
   - Cr√©ation de dashboards
   - Analyse des logs en temps r√©el

---

## üöÄ D√©marrage

### D√©marrer toute la stack

```bash
docker-compose up -d elasticsearch logstash kibana
```

### V√©rifier le statut

```bash
# Elasticsearch
curl http://localhost:9200/_cluster/health

# Kibana
curl http://localhost:5601/api/status

# Voir les logs
docker-compose logs -f logstash
```

### D√©marrer l'application avec ELK

```bash
# En mode production (logs envoy√©s √† Logstash)
SPRING_PROFILES_ACTIVE=prod ./mvnw spring-boot:run

# En mode dev (logs console uniquement)
SPRING_PROFILES_ACTIVE=dev ./mvnw spring-boot:run
```

---

## üìä Acc√®s aux interfaces

| Service | URL | Description |
|---------|-----|-------------|
| Kibana | http://localhost:5601 | Interface de visualisation |
| Elasticsearch | http://localhost:9200 | API REST Elasticsearch |
| Logstash | tcp://localhost:5044 | Endpoint TCP pour logs |

---

## üîç Configuration Kibana

### 1. Cr√©er un Index Pattern

1. Ouvrir Kibana: http://localhost:5601
2. Aller dans **Stack Management** ‚Üí **Index Patterns**
3. Cr√©er un pattern: `stockgestion-logs-*`
4. Choisir `@timestamp` comme champ de temps

### 2. Visualiser les logs

1. Aller dans **Discover**
2. S√©lectionner l'index pattern `stockgestion-logs-*`
3. Voir les logs en temps r√©el

### 3. Cr√©er des dashboards

Exemples de visualisations utiles:
- **Logs par niveau** (INFO, WARN, ERROR)
- **Timeline des erreurs**
- **Logs par service/classe**
- **Requ√™tes HTTP les plus fr√©quentes**

---

## üìù Configuration de l'application

### Fichiers modifi√©s

1. **compose.yaml**
   - Ajout des services Elasticsearch, Logstash, Kibana
   - Configuration r√©seau et volumes

2. **logstash.conf**
   - Pipeline de traitement des logs
   - Input: TCP port 5044 (JSON)
   - Output: Elasticsearch

3. **pom.xml**
   - D√©pendance: `logstash-logback-encoder`

4. **logback-spring.xml**
   - Appender LOGSTASH pour envoyer les logs
   - Profils: dev (console) vs prod (console + Logstash + fichier)

---

## üéØ Utilisation

### Exemple de logging dans le code

```java
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

@Service
public class ProductService {
    private static final Logger log = LoggerFactory.getLogger(ProductService.class);
    
    public Product createProduct(ProductDto dto) {
        log.info("Cr√©ation d'un produit: {}", dto.getSku());
        
        try {
            Product product = productRepository.save(product);
            log.info("Produit cr√©√© avec succ√®s: ID={}", product.getId());
            return product;
        } catch (Exception e) {
            log.error("Erreur lors de la cr√©ation du produit", e);
            throw e;
        }
    }
}
```

### Requ√™tes Kibana utiles

```
# Tous les logs ERROR
level:ERROR

# Logs d'un service sp√©cifique
logger_name:"com.example.stockgestion.services.ProductService"

# Logs avec exception
_exists_:stack_trace

# Logs des derni√®res 15 minutes
@timestamp:[now-15m TO now]
```

---

## üõ†Ô∏è Maintenance

### Nettoyer les anciens logs

```bash
# Supprimer les index de plus de 30 jours
curl -X DELETE "localhost:9200/stockgestion-logs-$(date -d '30 days ago' +%Y.%m.%d)"
```

### Arr√™ter la stack ELK

```bash
docker-compose stop elasticsearch logstash kibana
```

### Supprimer les donn√©es Elasticsearch

```bash
docker-compose down -v  # Supprime tous les volumes
# OU
docker volume rm stockgestion_es_data
```

---

## üîí S√©curit√© (Production)

‚ö†Ô∏è **IMPORTANT**: La configuration actuelle d√©sactive la s√©curit√© Elasticsearch pour simplifier le d√©veloppement.

Pour la production, activez la s√©curit√©:

```yaml
# compose.yaml - Elasticsearch
environment:
  - xpack.security.enabled=true
  - ELASTIC_PASSWORD=votre_mot_de_passe_fort
```

Et configurez l'authentification dans Logstash et Kibana.

---

## üìà Optimisation des performances

### Elasticsearch

- **M√©moire JVM**: Ajustez `ES_JAVA_OPTS` selon vos besoins (actuellement 1GB)
- **Nombre de shards**: Par d√©faut, 1 shard par index (suffisant pour dev)
- **Index Lifecycle**: Configurez la rotation automatique des index

### Logstash

- **M√©moire JVM**: Actuellement 512MB, ajustable via `LS_JAVA_OPTS`
- **Workers**: Ajoutez `pipeline.workers` dans logstash.conf si n√©cessaire

---

## üêõ D√©pannage

### Elasticsearch ne d√©marre pas

```bash
# V√©rifier les logs
docker-compose logs elasticsearch

# Probl√®me de m√©moire ? Augmenter vm.max_map_count
sudo sysctl -w vm.max_map_count=262144
```

### L'application n'envoie pas de logs

1. V√©rifier le profil actif: `SPRING_PROFILES_ACTIVE=prod`
2. V√©rifier que Logstash est d√©marr√©
3. V√©rifier les logs de l'application: `logs/stockgestion.log`

### Kibana n'affiche pas les logs

1. V√©rifier que l'index pattern existe
2. V√©rifier la p√©riode de temps s√©lectionn√©e
3. V√©rifier qu'Elasticsearch contient des donn√©es:
   ```bash
   curl http://localhost:9200/stockgestion-logs-*/_count
   ```

---

## üìö Ressources

- [Elasticsearch Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)
- [Logstash Documentation](https://www.elastic.co/guide/en/logstash/current/index.html)
- [Kibana Documentation](https://www.elastic.co/guide/en/kibana/current/index.html)
- [Logstash Logback Encoder](https://github.com/logfellow/logstash-logback-encoder)

---

## ‚úÖ Checklist de v√©rification

- [ ] Elasticsearch accessible sur port 9200
- [ ] Logstash accessible sur port 5044
- [ ] Kibana accessible sur port 5601
- [ ] Index pattern cr√©√© dans Kibana
- [ ] Application en mode `prod` envoie les logs
- [ ] Logs visibles dans Kibana Discover
- [ ] Dashboard cr√©√© (optionnel)
